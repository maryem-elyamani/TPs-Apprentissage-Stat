% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={TP-Arbres},
  pdfauthor={EL YAMANI MARYEM},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{TP-Arbres}
\author{EL YAMANI MARYEM}
\date{2023-09-28}

\begin{document}
\maketitle
\begin{abstract}
Ce TP porte sur l'étude des arbres de décision et de l'algorithme CART.
\end{abstract}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[breakable, interior hidden, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, enhanced, frame hidden, sharp corners]}{\end{tcolorbox}}\fi

\hypertarget{classification-avec-les-arbres}{%
\subsection{Classification avec les
arbres}\label{classification-avec-les-arbres}}

\hypertarget{question-1}{%
\subsubsection{\texorpdfstring{Question 1
}{Question 1 }}\label{question-1}}

Dans le cadre de la régression on peux utiliser la variance comme mesure
d'homogénéité.Car la variance quantifie la dispersion des valeurs de la
variable cible (Y) autour de leur moyenne.

\begin{itemize}
\item
  Si la variance est élevée : alors les valeurs de Y sont dispersées sur
  une plage plus large ce qui indique une hétérogénéité élevée entre les
  données.
\item
  Si la variance est faible : alors les valeurs de Y sont regroupées
  plus étroitement autour de leur moyenne ce qui indique une homogénéité
  plus élevée entre les données.
\end{itemize}

\hypertarget{question-2}{%
\subsubsection{\texorpdfstring{Question 2
}{Question 2 }}\label{question-2}}

Avec \texttt{scikit-learn} on peut créer des arbres de décision en
utilisant la classe tree \texttt{DecisionTreeClassifier}.

Pour la première simulation on génére un échantillon équilibré de taille
\(n = 456\) avec la fonction \texttt{rand\_checkers}. en veillant à
maintenir un équilibre entre les classes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tracer les données générées à partir de rand\_checkers}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{data[:, }\DecValTok{2}\NormalTok{] }\OperatorTok{+=} \DecValTok{1}
\NormalTok{scatter }\OperatorTok{=}\NormalTok{ sns.scatterplot(x}\OperatorTok{=}\NormalTok{data[:, }\DecValTok{0}\NormalTok{], y}\OperatorTok{=}\NormalTok{data[:, }\DecValTok{1}\NormalTok{], hue}\OperatorTok{=}\NormalTok{data[:, }\DecValTok{2}\NormalTok{], palette}\OperatorTok{=}\StringTok{"viridis"}\NormalTok{, s}\OperatorTok{=}\DecValTok{50}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"rand\_checkers generated datas"}\NormalTok{)}
\NormalTok{scatter.legend(title}\OperatorTok{=}\StringTok{"Classe"}\NormalTok{, bbox\_to\_anchor}\OperatorTok{=}\NormalTok{(}\FloatTok{1.05}\NormalTok{, }\DecValTok{1}\NormalTok{), loc}\OperatorTok{=}\StringTok{\textquotesingle{}upper left\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.grid(}\VariableTok{True}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{TP_files/figure-pdf/cell-3-output-1.pdf}

}

\end{figure}

On partitionne ensuite en 2 sous-ensembles pour avoir un ensemble
d'entrainement et un ensemble de test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ data[:, :}\DecValTok{2}\NormalTok{]}
\NormalTok{Y\_train }\OperatorTok{=}\NormalTok{ data[:, }\DecValTok{2}\NormalTok{].astype(}\BuiltInTok{int}\NormalTok{)}

\NormalTok{dt\_entropy }\OperatorTok{=}\NormalTok{ tree.DecisionTreeClassifier(criterion}\OperatorTok{=}\StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{)}
\NormalTok{dt\_gini }\OperatorTok{=}\NormalTok{ tree.DecisionTreeClassifier(criterion}\OperatorTok{=}\StringTok{\textquotesingle{}gini\textquotesingle{}}\NormalTok{)}

\CommentTok{\#calcule les performances du score sur les données d\textquotesingle{}apprentissage}

\NormalTok{dt\_gini.fit(X\_train, Y\_train)}
\NormalTok{dt\_entropy.fit(X\_train, Y\_train)}
\NormalTok{train\_error\_gini }\OperatorTok{=} \DecValTok{1} \OperatorTok{{-}}\NormalTok{ dt\_gini.score(X\_train, Y\_train)}
\NormalTok{train\_error\_entropy }\OperatorTok{=} \DecValTok{1} \OperatorTok{{-}}\NormalTok{ dt\_entropy.score(X\_train, Y\_train)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"L\textquotesingle{}erreur avec le critère de Gini est :"}\NormalTok{, train\_error\_gini)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"L\textquotesingle{}erreur avec le critère d\textquotesingle{}entropie est :"}\NormalTok{,train\_error\_entropy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
L'erreur avec le critère de Gini est : 0.0
L'erreur avec le critère d'entropie est : 0.0
\end{verbatim}

Un résultat d'erreur de classification de 0 signifie que le modèle a
correctement classé toutes les observations dans l'ensemble
d'entraînement ce qui est excellent en termes de performance de
classification.

\begin{verbatim}
    Max Depth  Error (Entropy)  Error (Gini)
0           1         0.736607      0.736607
1           2         0.729911      0.729911
2           3         0.703125      0.703125
3           4         0.647321      0.638393
4           5         0.529018      0.537946
5           6         0.370536      0.361607
6           7         0.234375      0.238839
7           8         0.133929      0.125000
8           9         0.075893      0.071429
9          10         0.037946      0.037946
10         11         0.031250      0.022321
11         12         0.013393      0.011161
\end{verbatim}

\includegraphics{TP_files/figure-pdf/cell-5-output-2.pdf}

\includegraphics{TP_files/figure-pdf/cell-5-output-3.pdf}

Les erreurs des critères d'entropie et de Gini diminuent à mesure que la
profondeur maximale de l'arbre augmente ce qui suggère une amélioration
de la performance du modèle avec une complexité croissante.

Le choix de la profondeur optimale est crucial pour l'équilibre entre
adaptation aux données et généralisation. une amélioration de la
performance du modèle avec une complexité croissante.

\hypertarget{question-3}{%
\subsubsection{\texorpdfstring{Question 3
}{Question 3 }}\label{question-3}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt\_entropy.max\_depth }\OperatorTok{=}\NormalTok{ np.argmin(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{scores\_entropy)}\OperatorTok{+}\DecValTok{1}
\NormalTok{plt.figure()}
\NormalTok{frontiere(}\KeywordTok{lambda}\NormalTok{ x: dt\_entropy.predict(x.reshape((}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{))),X\_train, Y\_train, step}\OperatorTok{=}\DecValTok{100}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Best frontier with entropy criterion"}\NormalTok{)}
\NormalTok{plt.draw()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Best scores with entropy criterion: "}\NormalTok{, dt\_entropy.score(X\_train, Y\_train))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Best scores with entropy criterion:  0.9866071428571429
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{TP_files/figure-pdf/frontiere-output-2.pdf}

}

\caption{Frontières pour la meilleur profondeur (entropie)}

\end{figure}

Nous allons maintenant visualiser la classification obtenue en utilisant
la profondeur de l'arbre qui minimise les erreurs basées sur l'entropie.
Pour ce faire, nous utiliserons la fonction frontiere() du fichier
source. Il est à noter que pour une profondeur de 12, nous avons observé
une absence d'erreur, ce qui signifie que le modèle a obtenu un score
parfait de 1 (score = 1 - erreur). Cette classification précise met en
évidence l'efficacité remarquable de l'arbre de décision dans
l'apprentissage des données.

\hypertarget{question-4}{%
\subsubsection{\texorpdfstring{Question 4
}{Question 4 }}\label{question-4}}

Nous allons utiliser la fonction export\_graphviz() du module tree pour
générer un graphique représentant l'arbre résultant de la question
précédente. Ce graphique sera sauvegardé dans le fichier `graphs' .

Un arbre de décision est une structure conceptuelle relativement simple
composée d'un nœud racine, suivi de deux nœuds enfants à chaque niveau.
Chaque nœud non-terminal (qui n'est pas une feuille) possède également
deux nœuds enfants, et cette structure se poursuit jusqu'à ce que nous
atteignions les nœuds terminaux, également appelés feuilles, qui
contiennent les décisions finales. Lorsque nous évaluons un point de
données à travers l'arbre, si une condition au nœud k est vraie pour ce
point, nous suivons la branche de gauche ; sinon, nous empruntons la
branche de droite. Cette logique se répète jusqu'à ce que nous
atteignions une feuille, qui détermine la décision ou la classe associée
à ce point de données.

\hypertarget{question-5}{%
\subsubsection{\texorpdfstring{Question 5
}{Question 5 }}\label{question-5}}

Nous allons maintenant évaluer la précision de notre arbre en calculant
son taux d'erreur sur un nouvel échantillon de 160 données générées à
l'aide de la fonction \texttt{rand\_checkers}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Créer un nouvel échantillon de test avec 160 données (40 de chaque classe)}
\NormalTok{data\_test }\OperatorTok{=}\NormalTok{ rand\_checkers(n1}\OperatorTok{=}\DecValTok{40}\NormalTok{, n2}\OperatorTok{=}\DecValTok{40}\NormalTok{, n3}\OperatorTok{=}\DecValTok{40}\NormalTok{, n4}\OperatorTok{=}\DecValTok{40}\NormalTok{)}
\NormalTok{X\_test }\OperatorTok{=}\NormalTok{ data\_test[:,:}\DecValTok{2}\NormalTok{]}
\NormalTok{Y\_test }\OperatorTok{=}\NormalTok{ np.asarray(data\_test[:,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{)}

\NormalTok{dmax }\OperatorTok{=} \DecValTok{30}                             
\NormalTok{scores\_entropy }\OperatorTok{=}\NormalTok{ np.zeros(dmax)}
\NormalTok{scores\_gini }\OperatorTok{=}\NormalTok{ np.zeros(dmax)}


\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(dmax):}
    \CommentTok{\# Critère : entropie}
\NormalTok{    dt\_entropy }\OperatorTok{=}\NormalTok{ tree.DecisionTreeClassifier(criterion}\OperatorTok{=}\StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{, }
\NormalTok{                                             max\_depth}\OperatorTok{=}\NormalTok{i}\OperatorTok{+}\DecValTok{1}\NormalTok{)}
\NormalTok{    dt\_entropy.fit(X\_test,Y\_test)}
\NormalTok{    scores\_entropy[i] }\OperatorTok{=}\NormalTok{ dt\_entropy.score(X\_test, Y\_test)}

    \CommentTok{\# Critère : indice de Gini}
\NormalTok{    dt\_gini }\OperatorTok{=}\NormalTok{ tree.DecisionTreeClassifier(criterion}\OperatorTok{=}\StringTok{\textquotesingle{}gini\textquotesingle{}}\NormalTok{, }
\NormalTok{                                          max\_depth}\OperatorTok{=}\NormalTok{i}\OperatorTok{+}\DecValTok{1}\NormalTok{)}
\NormalTok{    dt\_gini.fit(X\_test,Y\_test)}
\NormalTok{    scores\_gini[i] }\OperatorTok{=}\NormalTok{ dt\_gini.score(X\_test,Y\_test)}

\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\FloatTok{3.2}\NormalTok{))}
\NormalTok{plt.plot(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{scores\_entropy)}
\NormalTok{plt.plot(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{scores\_gini)}
\NormalTok{plt.legend([}\StringTok{\textquotesingle{}Entropie\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Gini\textquotesingle{}}\NormalTok{])}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Profondeur maximale\textquotesingle{}}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Taux d\textquotesingle{}erreur"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Courbe d\textquotesingle{}erreur test"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, "Courbe d'erreur test")
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{TP_files/figure-pdf/cell-8-output-2.pdf}

}

\end{figure}

Lors de cette analyse, nous avons remarqué une diminution du taux
d'erreur lorsque la profondeur de l'arbre est initialement faible,
suivie d'une stabilisation après avoir atteint un certain niveau de
profondeur. Cette observation suggère que l'augmentation supplémentaire
de la profondeur de l'arbre ne semble pas conduire à une amélioration
significative du taux d'erreur.



\end{document}
