---
title: "TP-Arbres"
format:
  html: 
    papersize: letter
    author: EL YAMANI MARYEM
    date: 2023/09/28
    number-sections: true
    colorlinks: true
    abstract: Ce TP porte sur l'étude des arbres de décision et de l'algorithme CART.
    fig-align: center
---

## Classification avec les arbres 
<a name="Classification avec les arbres"></a>

### Question 1 <a name="Question 1"></a>

Dans le cadre de la régression on peux utiliser la variance comme mesure d'homogénéité.Car la variance quantifie la dispersion des valeurs de la variable cible (Y) autour de leur moyenne.

- Si la variance est élevée : alors les valeurs de Y sont dispersées sur une plage plus large ce qui indique une hétérogénéité élevée entre les données.

- Si la variance est faible : alors les valeurs de Y sont regroupées plus étroitement autour de leur moyenne ce qui indique une homogénéité plus élevée entre les données.

### Question 2 <a name="Question 2"></a>

Avec `scikit-learn` on peut créer des arbres de décision en utilisant la classe tree `DecisionTreeClassifier`.

Pour la première simulation on génére un échantillon équilibré de taille $n = 456$ avec la fonction `rand_checkers`. en veillant à maintenir un équilibre entre les classes.

```{python}
#| echo: false
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rc
import sys
import pandas as pd
sys.path.append('./Code/')
from tp_arbres_source import *
from sklearn import tree, datasets, model_selection
from sklearn.model_selection import cross_val_score, learning_curve 

# Simulation de l'échantillon
n = 456
data = rand_checkers(n1=n//4, n2=n//4, n3=n//4, n4=n//4)

# Configuration des paramètres de base de matplotlib
plt.rc('font', family='sans-serif')
plt.rc('font', serif=['Computer Modern Roman'])
plt.rc('axes', labelsize=6)
plt.rc('font', size=12)
plt.rc('legend', fontsize=12)
plt.rc('text', usetex=False)
plt.rc('figure', figsize=(10, 12))

# Configuration de seaborn
sns.set_context("poster")
sns.set_palette("colorblind")
sns.set_style("white")
```

```{python}
# Tracer les données générées à partir de rand_checkers
plt.figure(figsize=(8, 6))
data[:, 2] += 1
scatter = sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=data[:, 2], palette="viridis", s=50)
plt.title("rand_checkers generated datas")
scatter.legend(title="Classe", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.show()
```

On partitionne ensuite en 2 sous-ensembles pour avoir un ensemble d'entrainement et un ensemble de test. 

```{python}
X_train = data[:, :2]
Y_train = data[:, 2].astype(int)

dt_entropy = tree.DecisionTreeClassifier(criterion='entropy')
dt_gini = tree.DecisionTreeClassifier(criterion='gini')

#calcule les performances du score sur les données d'apprentissage

dt_gini.fit(X_train, Y_train)
dt_entropy.fit(X_train, Y_train)
train_error_gini = 1 - dt_gini.score(X_train, Y_train)
train_error_entropy = 1 - dt_entropy.score(X_train, Y_train)
print("L'erreur avec le critère de Gini est :", train_error_gini)
print("L'erreur avec le critère d'entropie est :",train_error_entropy)
```

Un résultat d'erreur de classification de 0 signifie que le modèle a correctement classé toutes les observations dans l'ensemble d'entraînement ce qui est excellent en termes de performance de classification.


```{python}
#| echo: FALSE
np.random.seed(678)
dmax = 12
scores_entropy = np.zeros(dmax)
scores_gini = np.zeros(dmax)

error_data = {'Max Depth': [], 'Error (Entropy)': [], 'Error (Gini)': []}

plt.figure(figsize=(15, 10))
for i in range(dmax):
    dt_entropy = tree.DecisionTreeClassifier(criterion='entropy', max_depth=i+1)
    dt_entropy.fit(X_train, Y_train)
    scores_entropy[i] = 1 - dt_entropy.score(X_train, Y_train)

    dt_gini = tree.DecisionTreeClassifier(criterion='gini', max_depth=i+1)
    dt_gini.fit(X_train, Y_train)
    scores_gini[i] = 1 - dt_gini.score(X_train, Y_train)

    plt.subplot(3, 4, i + 1)
    frontiere(lambda x: dt_gini.predict(x.reshape((1, -1))), X_train, Y_train,
              step=50, samples=False)
    
    error_data['Max Depth'].append(i+1)
    error_data['Error (Entropy)'].append(scores_entropy[i])
    error_data['Error (Gini)'].append(scores_gini[i])
    
plt.draw()

plt.figure()
plt.plot(range(1, dmax + 1), scores_entropy, label='entropy')
plt.plot(range(1, dmax + 1), scores_gini, label='gini')
plt.xlabel('Max depth')
plt.ylabel('Error')
plt.title('Error with entropy and gini criterion')
plt.legend()
plt.draw()

# Créer un DataFrame à partir des données d'erreur
error_df = pd.DataFrame(error_data)
print(error_df)
```

Les erreurs des critères d'entropie et de Gini diminuent à mesure que la profondeur maximale de l'arbre augmente ce qui suggère une amélioration de la performance du modèle avec une complexité croissante.

Le choix de la profondeur optimale est crucial pour l'équilibre entre adaptation aux données et généralisation. une amélioration de la performance du modèle avec une complexité croissante.

### Question 3 <a name="Question 3"></a>

```{python}
#| label: frontiere
#| fig-cap: "Frontières pour la meilleur profondeur (entropie)"
dt_entropy.max_depth = np.argmin(1-scores_entropy)+1
plt.figure()
frontiere(lambda x: dt_entropy.predict(x.reshape((1, -1))),X_train, Y_train, step=100)
plt.title("Best frontier with entropy criterion")
plt.draw()
print("Best scores with entropy criterion: ", dt_entropy.score(X_train, Y_train))
```

Nous allons maintenant visualiser la classification obtenue en utilisant la profondeur de l'arbre qui minimise les erreurs basées sur l'entropie. Pour ce faire, nous utiliserons la fonction frontiere() du fichier source. Il est à noter que pour une profondeur de 12, nous avons observé une absence d'erreur, ce qui signifie que le modèle a obtenu un score parfait de 1 (score = 1 - erreur). Cette classification précise met en évidence l'efficacité remarquable de l'arbre de décision dans l'apprentissage des données.

### Question 4 <a name="Question 4"></a>

Nous allons utiliser la fonction export_graphviz() du module tree pour générer un graphique représentant l'arbre résultant de la question précédente. 
Ce graphique sera sauvegardé dans le fichier 'graphs' .

```{python}
#| eval: FALSE
#| echo: FALSE
import graphviz
#dot_data = tree.export_graphviz(dt_entropy, out_file=None)
#graph = graphviz.Source(dot_data)
#graph.render("./Graphs/decisiontree")

```

Un arbre de décision est une structure conceptuelle relativement simple composée d'un nœud racine, suivi de deux nœuds enfants à chaque niveau. Chaque nœud non-terminal (qui n'est pas une feuille) possède également deux nœuds enfants, et cette structure se poursuit jusqu'à ce que nous atteignions les nœuds terminaux, également appelés feuilles, qui contiennent les décisions finales. Lorsque nous évaluons un point de données à travers l'arbre, si une condition au nœud k est vraie pour ce point, nous suivons la branche de gauche ; sinon, nous empruntons la branche de droite. Cette logique se répète jusqu'à ce que nous atteignions une feuille, qui détermine la décision ou la classe associée à ce point de données.

<img src='decisiontree.png' width=800>
### Question 5 <a name="Question 5"></a>

Nous allons maintenant évaluer la précision de notre arbre en calculant son taux d'erreur sur un nouvel échantillon de 160 données générées à l'aide de la fonction `rand_checkers`.






```{python}
#| message: FALSE
# Créer un nouvel échantillon de test avec 160 données (40 de chaque classe)
data_test = rand_checkers(n1=40, n2=40, n3=40, n4=40)
X_test = data_test[:,:2]
Y_test = np.asarray(data_test[:,-1], dtype=int)

dmax = 30                             
scores_entropy = np.zeros(dmax)
scores_gini = np.zeros(dmax)


for i in range(dmax):
    # Critère : entropie
    dt_entropy = tree.DecisionTreeClassifier(criterion='entropy', 
                                             max_depth=i+1)
    dt_entropy.fit(X_test,Y_test)
    scores_entropy[i] = dt_entropy.score(X_test, Y_test)

    # Critère : indice de Gini
    dt_gini = tree.DecisionTreeClassifier(criterion='gini', 
                                          max_depth=i+1)
    dt_gini.fit(X_test,Y_test)
    scores_gini[i] = dt_gini.score(X_test,Y_test)

plt.figure(figsize=(6,3.2))
plt.plot(1-scores_entropy)
plt.plot(1-scores_gini)
plt.legend(['Entropie', 'Gini'])
plt.xlabel('Profondeur maximale', fontsize=12)
plt.ylabel("Taux d'erreur", fontsize=12)
plt.title("Courbe d'erreur test", fontsize=15)
```


Lors de cette analyse, nous avons remarqué une diminution du taux d'erreur lorsque la profondeur de l'arbre est initialement faible, suivie d'une stabilisation après avoir atteint un certain niveau de profondeur. Cette observation suggère que l'augmentation supplémentaire de la profondeur de l'arbre ne semble pas conduire à une amélioration significative du taux d'erreur.

